{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's import the Python bindings, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.10'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metapy.__version__ # you will want your version to be >= to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you would like to, you can inform MeTA to output log data to stderr like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metapy.log_to_stderr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's download a list of stopwords and a sample dataset to begin exploring MeTA's topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -N https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -N https://meta-toolkit.org/data/2016-01-26/ceeaus.tar.gz\n",
    "!tar xf ceeaus.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will need to index our data to proceed. We eventually want to be able to extract the bag-of-words representation for our individual documents, so we will want a `ForwardIndex` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Counting lines in file: [=================================] 100% ETA 00:00:00 \n",
      "1525207335: [info]     Creating forward index: ceeaus-idx/fwd (/tmp/pip-req-build-m473bt6z/deps/meta/src/index/forward_index.cpp:239)\n",
      " > Tokenizing Docs: [========================================] 100% ETA 00:00:00 \n",
      " > Merging: [================================================] 100% ETA 00:00:00 \n",
      "1525207335: [info]     Done creating index: ceeaus-idx/fwd (/tmp/pip-req-build-m473bt6z/deps/meta/src/index/forward_index.cpp:278)\n"
     ]
    }
   ],
   "source": [
    "fidx = metapy.index.make_forward_index('ceeaus-config.toml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just like in classification, the feature set used for the topic modeling will be the feature set used at the time of indexing, so if you want to play with a different set of features (like bigram words), you will need to re-index your data.\n",
    "\n",
    "For now, we've just stuck with the default filter chain for unigram words, so we're operating in the traditional bag-of-words space.\n",
    "\n",
    "Let's load our documents into memory to run the topic model inference now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r\n",
      " > Loading instances into memory: [>                         ]   0% ETA 00:00:00 \r\n",
      " > Loading instances into memory: [==========================] 100% ETA 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "dset = metapy.learn.Dataset(fidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's try to find some topics for this dataset. To do so, we're going to use a generative model called a topic model.\n",
    "\n",
    "There are many different topic models in the literature, but the most commonly used topic model is Latent Dirichlet Allocation. Here, we propose that there are K topics (represented with a categorical distribution over words) $\\phi_k$ from which all of our documents are genereated. These K topics are modeled as being sampled from a Dirichlet distribution with parameter $\\vec{\\alpha}$. Then, to generate a document $d$, we first sample a distribution over the K topics $\\theta_d$ from another Dirichlet distribution with parameter $\\vec{\\beta}$. Then, for each word in this document, we first sample a topic identifier $z \\sim \\theta_d$ and then the word by drawing from the topic we selected ($w \\sim \\phi_z$). Refer to the [Wikipedia article on LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) for more information.\n",
    "\n",
    "The goal of running inference for an LDA model is to infer the latent variables $\\phi_k$ and $\\theta_d$ for all of the $K$ topics and $D$ documents, respectively. MeTA provides a number of different inference algorithms for LDA, as each one entails a different set of trade-offs (inference in LDA is intractable, so all inference algorithms are approximations; different algorithms entail different approximation guarantees, running times, and required memroy consumption). For now, let's run a Variational Infernce algorithm called CVB0 to find two topics. (In practice you will likely be finding many more topics than just two, but this is a very small toy dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialization: [============================================] 100% ETA 00:00:00 \n",
      "Iteration 1 maximum change in gamma: 1.78732                                     \n",
      "Iteration 2 maximum change in gamma: 0.397587                                    \n",
      "Iteration 3 maximum change in gamma: 0.585143                                    \n",
      "Iteration 4 maximum change in gamma: 0.639287                                    \n",
      "Iteration 5 maximum change in gamma: 0.543676                                    \n",
      "Iteration 6 maximum change in gamma: 1.04887                                     \n",
      "Iteration 7 maximum change in gamma: 1.26393                                     \n",
      "Iteration 8 maximum change in gamma: 1.43444                                     \n",
      "Iteration 9 maximum change in gamma: 1.36808                                     \n",
      "Iteration 10 maximum change in gamma: 1.39221                                    \n",
      "Iteration 11 maximum change in gamma: 1.3227                                     \n",
      "Iteration 12 maximum change in gamma: 1.21134                                    \n",
      "Iteration 13 maximum change in gamma: 0.759168                                   \n",
      "Iteration 14 maximum change in gamma: 0.805964                                   \n",
      "Iteration 15 maximum change in gamma: 0.802893                                   \n",
      "Iteration 16 maximum change in gamma: 0.571095                                   \n",
      "Iteration 17 maximum change in gamma: 0.516594                                   \n",
      "Iteration 18 maximum change in gamma: 0.523159                                   \n",
      "Iteration 19 maximum change in gamma: 0.275293                                   \n",
      "Iteration 20 maximum change in gamma: 0.228512                                   \n",
      "Iteration 21 maximum change in gamma: 0.237871                                   \n",
      "Iteration 22 maximum change in gamma: 0.203393                                   \n",
      "Iteration 23 maximum change in gamma: 0.214538                                   \n",
      "Iteration 24 maximum change in gamma: 0.205745                                   \n",
      "Iteration 25 maximum change in gamma: 0.169748                                   \n",
      "Iteration 26 maximum change in gamma: 0.122061                                   \n",
      "Iteration 27 maximum change in gamma: 0.119245                                   \n",
      "Iteration 28 maximum change in gamma: 0.129667                                   \n",
      "Iteration 29 maximum change in gamma: 0.134692                                   \n",
      "Iteration 30 maximum change in gamma: 0.132623                                   \n",
      "Iteration 31 maximum change in gamma: 0.129393                                   \n",
      "Iteration 32 maximum change in gamma: 0.151032                                   \n",
      "Iteration 33 maximum change in gamma: 0.164985                                   \n",
      "Iteration 34 maximum change in gamma: 0.165914                                   \n",
      "Iteration 35 maximum change in gamma: 0.152212                                   \n",
      "Iteration 36 maximum change in gamma: 0.142474                                   \n",
      "Iteration 37 maximum change in gamma: 0.171535                                   \n",
      "Iteration 38 maximum change in gamma: 0.191064                                   \n",
      "Iteration 39 maximum change in gamma: 0.192934                                   \n",
      "Iteration 40 maximum change in gamma: 0.202834                                   \n",
      "Iteration 41 maximum change in gamma: 0.220982                                   \n",
      "Iteration 42 maximum change in gamma: 0.210217                                   \n",
      "Iteration 43 maximum change in gamma: 0.213731                                   \n",
      "Iteration 44 maximum change in gamma: 0.192772                                   \n",
      "Iteration 45 maximum change in gamma: 0.15127                                    \n",
      "Iteration 46 maximum change in gamma: 0.105296                                   \n",
      "Iteration 47 maximum change in gamma: 0.0670212                                  \n",
      "Iteration 48 maximum change in gamma: 0.0402179                                  \n",
      "Iteration 49 maximum change in gamma: 0.0360913                                  \n",
      "Iteration 50 maximum change in gamma: 0.0366095                                  \n",
      "Iteration 51 maximum change in gamma: 0.037                                      \n",
      "Iteration 52 maximum change in gamma: 0.0372273                                  \n",
      "Iteration 53 maximum change in gamma: 0.0374239                                  \n",
      "Iteration 54 maximum change in gamma: 0.0379712                                  \n",
      "Iteration 55 maximum change in gamma: 0.0393836                                  \n",
      "Iteration 56 maximum change in gamma: 0.0430323                                  \n",
      "Iteration 57 maximum change in gamma: 0.0467638                                  \n",
      "Iteration 58 maximum change in gamma: 0.0504449                                  \n",
      "Iteration 59 maximum change in gamma: 0.0539003                                  \n",
      "Iteration 60 maximum change in gamma: 0.0569188                                   \n",
      "Iteration 61 maximum change in gamma: 0.0592677                                  \n",
      "Iteration 62 maximum change in gamma: 0.0607189                                  \n",
      "Iteration 63 maximum change in gamma: 0.0610821                                  \n",
      "Iteration 64 maximum change in gamma: 0.0616854                                  \n",
      "Iteration 65 maximum change in gamma: 0.0622165                                  \n",
      "Iteration 66 maximum change in gamma: 0.0614795                                  \n",
      "Iteration 67 maximum change in gamma: 0.0594567                                  \n",
      "Iteration 68 maximum change in gamma: 0.0562496                                  \n",
      "Iteration 69 maximum change in gamma: 0.0520662                                  \n",
      "Iteration 70 maximum change in gamma: 0.0471889                                  \n",
      "Iteration 71 maximum change in gamma: 0.0419296                                  \n",
      "Iteration 72 maximum change in gamma: 0.0365863                                  \n",
      "Iteration 73 maximum change in gamma: 0.0314094                                  \n",
      "Iteration 74 maximum change in gamma: 0.0265839                                  \n",
      "Iteration 75 maximum change in gamma: 0.0222263                                  \n",
      "Iteration 76 maximum change in gamma: 0.0183923                                  \n",
      "Iteration 77 maximum change in gamma: 0.0150897                                  \n",
      "Iteration 78 maximum change in gamma: 0.0122935                                  \n",
      "Iteration 79 maximum change in gamma: 0.00995884                                 \n",
      "Iteration 80 maximum change in gamma: 0.00803111                                 \n",
      "Iteration 81 maximum change in gamma: 0.00645346                                 \n",
      "Iteration 82 maximum change in gamma: 0.00517133                                 \n",
      "Iteration 83 maximum change in gamma: 0.0041351                                  \n",
      "Iteration 84 maximum change in gamma: 0.00330117                                 \n",
      "Iteration 85 maximum change in gamma: 0.00263226                                 \n",
      "Iteration 86 maximum change in gamma: 0.00209708                                 \n",
      "Iteration 87 maximum change in gamma: 0.00166969                                 \n",
      "Iteration 88 maximum change in gamma: 0.00132887                                 \n",
      "Iteration 89 maximum change in gamma: 0.00121476                                 \n",
      "Iteration 90 maximum change in gamma: 0.00116639                                 \n",
      "Iteration 91 maximum change in gamma: 0.00111979                                 \n",
      "Iteration 92 maximum change in gamma: 0.00107491                                 \n",
      "Iteration 93 maximum change in gamma: 0.00103169                                 \n",
      "Iteration 94 maximum change in gamma: 0.00099009                                 \n",
      "1525207346: [info]     Finished maximum iterations, or found convergence! (/tmp/pip-req-build-m473bt6z/deps/meta/src/topics/lda_cvb.cpp:60)\n"
     ]
    }
   ],
   "source": [
    "lda_inf = metapy.topics.LDACollapsedVB(dset, num_topics=2, alpha=1.0, beta=0.01)\n",
    "lda_inf.run(num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above ran the CVB0 algorithm for 1000 iterations, or until an algorithm-specific convergence criterion was met. Now let's save the current estimate for our topics and topic proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda_inf.save('lda-cvb0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can interrogate the topic inference results by using the `TopicModel` query class. Let's load our inference results back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r\n",
      " > Loading topic term probabilities: [===========>           ]  50% ETA 00:00:00 \r\n",
      " > Loading topic term probabilities: [=======================] 100% ETA 00:00:00 \n",
      " \r\n",
      " > Loading document topic probabilities: [>                  ]   0% ETA 00:00:00 \r\n",
      " > Loading document topic probabilities: [===================] 100% ETA 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "model = metapy.topics.TopicModel('lda-cvb0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's have a look at our topics. A typical way of doing this is to print the top $k$ words in each topic, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3341, 0.1311039703325231),\n",
       " (3045, 0.05434932034406297),\n",
       " (2677, 0.036780095760011296),\n",
       " (3346, 0.033492639884972024),\n",
       " (281, 0.022530673690313033),\n",
       " (3729, 0.015620482424303144),\n",
       " (1953, 0.012780918673797484),\n",
       " (707, 0.012635069663149857),\n",
       " (592, 0.011987183284170312),\n",
       " (2448, 0.01131774038055637)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.top_k(tid=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The models operate on term ids instead of raw text strings, so let's convert this to a human readable format by using the vocabulary contained in our `ForwardIndex` to map the term ids to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 0.1311039703325231),\n",
       " ('restaur', 0.05434932034406297),\n",
       " ('peopl', 0.036780095760011296),\n",
       " ('smoker', 0.033492639884972024),\n",
       " ('ban', 0.022530673690313033),\n",
       " ('think', 0.015620482424303144),\n",
       " ('japan', 0.012780918673797484),\n",
       " ('complet', 0.012635069663149857),\n",
       " ('cigarett', 0.011987183284170312),\n",
       " ('non', 0.01131774038055637)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 0.06705646364996719),\n",
       " ('job', 0.056059299650279136),\n",
       " ('part', 0.05222306274365633),\n",
       " ('student', 0.046429384401537266),\n",
       " ('colleg', 0.03488140708901945),\n",
       " ('work', 0.029067480910345566),\n",
       " ('money', 0.02885021953621179),\n",
       " ('think', 0.0223313502030152),\n",
       " ('import', 0.02075570151328543),\n",
       " ('studi', 0.015483035500804767)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can pretty clearly see that this particular dataset was about two major issues: smoking in public and part time jobs for students. This dataset is actually a collection of essays written by students, and there just so happen to be two different topics they can choose from!\n",
    "\n",
    "The topics are pretty clear in this case, but in some cases it is also useful to score the terms in a topic using some function of the probability of the word in the topic and the probability of the word in the other topics. Intuitively, we might want to select words from each topic that best reflect that topic's content by picking words that both have high probability in that topic **and** have low probability in the other topics. In other words, we want to balance between high probability terms and highly specific terms (this is kind of like a tf-idf weighting). One such scoring function is provided by the toolkit in `BLTermScorer`, which implements a scoring function proposed by Blei and Lafferty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 0.8741642954704982),\n",
       " ('restaur', 0.3174635157613981),\n",
       " ('smoker', 0.20060264558827434),\n",
       " ('ban', 0.12853037061168004),\n",
       " ('cigarett', 0.06557603445386917),\n",
       " ('non', 0.06128422163678793),\n",
       " ('complet', 0.06105372135982501),\n",
       " ('japan', 0.05846453427778453),\n",
       " ('health', 0.05054834419152887),\n",
       " ('seat', 0.04533989518585457)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = metapy.topics.BLTermScorer(model)\n",
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=0, scorer=scorer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('job', 0.34822052999812314),\n",
       " ('part', 0.31311071220949405),\n",
       " ('student', 0.28328931730015805),\n",
       " ('colleg', 0.20808996037214555),\n",
       " ('time', 0.17797810952278859),\n",
       " ('money', 0.16234681881133195),\n",
       " ('work', 0.15585089245616857),\n",
       " ('studi', 0.08228292617409116),\n",
       " ('learn', 0.06491899302248028),\n",
       " ('experi', 0.05494526738519956)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=1, scorer=scorer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we can see that the uninformative word stem \"think\" was downweighted from the word list from each topic, since it had relatively high probability in either topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also see the inferred topic distribution for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metapy.stats.Multinomial {0: 0.021341, 1: 0.978659}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_distribution(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It looks like our first document was written by a student who chose the part-time job essay topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metapy.stats.Multinomial {0: 0.978797, 1: 0.021203}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_distribution(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "...whereas this document looks like it was written by a student who chose the public smoking essay topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also infer topics for a brand new document. First, let's create the document and use the forward index we loaded before to convert it to a feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc = metapy.index.Document()\n",
    "doc.content(\"I think smoking in public is bad for others' health.\")\n",
    "fvec = fidx.tokenize(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's load a topic model inferencer that uses the same CVB inference method we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r\n",
      " > Loading topic term probabilities: [===========>           ]  50% ETA 00:00:00 \r\n",
      " > Loading topic term probabilities: [=======================] 100% ETA 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "inferencer = metapy.topics.CVBInferencer('lda-cvb0.phi.bin', alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's infer the topic proportions for the new document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<metapy.stats.Multinomial {0: 0.814392, 1: 0.185608}>\n"
     ]
    }
   ],
   "source": [
    "proportions = inferencer.infer(fvec, max_iters=20, convergence=1e-4)\n",
    "print(proportions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
